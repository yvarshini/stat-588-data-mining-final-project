{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 16:49:53.037423: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import requests\n",
    "from io import StringIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifiers(X, y, output=None, n_splits=10, random_state=42, epochs=50, batch_size=16, learning_rate=0.001):\n",
    "    if output == None:\n",
    "        output = len(np.unique(y))\n",
    "        \n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "    # KFold\n",
    "    #kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    mse_list_knn = []\n",
    "    mse_list_neural_network = []\n",
    "    mse_list_extra_trees = []\n",
    "    accuracy_list_knn = []\n",
    "    accuracy_list_neural_network = []\n",
    "    accuracy_list_extra_trees = []\n",
    "\n",
    "    # Cross-validation on the training set\n",
    "    for train_index, val_index in kf.split(X_train_all, y_train_all):\n",
    "        X_train, X_val = X_train_all[train_index], X_train_all[val_index]\n",
    "        y_train, y_val = y_train_all[train_index], y_train_all[val_index]\n",
    "\n",
    "        # KNN\n",
    "        knn = KNeighborsClassifier(n_neighbors=3)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred_knn = knn.predict(X_val)\n",
    "        mse_knn = mean_squared_error(y_val, y_pred_knn)\n",
    "        mse_list_knn.append(mse_knn)\n",
    "        accuracy_knn = accuracy_score(y_val, y_pred_knn)\n",
    "        accuracy_list_knn.append(accuracy_knn)\n",
    "\n",
    "        # Neural Networks\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        #model.add(Dense(len(np.unique(y)), activation='softmax'))\n",
    "        #model.add(Dense(len(np.unique(y_train)), activation='softmax'))\n",
    "        model.add(Dense(output, activation='softmax'))\n",
    "\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        y_train_categorical = to_categorical(y_train)\n",
    "        y_val_categorical = to_categorical(y_val)\n",
    "        model.fit(X_train, y_train_categorical, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        y_pred_val_neural_network = np.argmax(model.predict(X_val), axis=1)\n",
    "        mse_neural_network = mean_squared_error(y_val, y_pred_val_neural_network)\n",
    "        mse_list_neural_network.append(mse_neural_network)\n",
    "        accuracy_neural_network = accuracy_score(y_val, y_pred_val_neural_network)\n",
    "        accuracy_list_neural_network.append(accuracy_neural_network)\n",
    "\n",
    "        # ExtraTreesClassifier\n",
    "        extra_trees = ExtraTreesClassifier(n_estimators=100, random_state=random_state)\n",
    "        extra_trees.fit(X_train, y_train)\n",
    "        y_pred_extra_trees = extra_trees.predict(X_val)\n",
    "        mse_extra_trees = mean_squared_error(y_val, y_pred_extra_trees)\n",
    "        mse_list_extra_trees.append(mse_extra_trees)\n",
    "        accuracy_extra_trees = accuracy_score(y_val, y_pred_extra_trees)\n",
    "        accuracy_list_extra_trees.append(accuracy_extra_trees)\n",
    "\n",
    "\n",
    "    y_pred_knn_test = knn.predict(X_test_all)\n",
    "    test_error_knn = 1 - accuracy_score(y_test_all, y_pred_knn_test)\n",
    "    test_mse_knn = mean_squared_error(y_test_all, y_pred_knn_test)\n",
    "\n",
    "\n",
    "    y_pred_test_neural_network = np.argmax(model.predict(X_test_all), axis=1)\n",
    "    test_error_neural_network = 1 - accuracy_score(y_test_all, y_pred_test_neural_network)\n",
    "    test_mse_neural_network = mean_squared_error(y_test_all, y_pred_test_neural_network)\n",
    "\n",
    "\n",
    "    y_pred_extra_trees_test = extra_trees.predict(X_test_all)\n",
    "    test_error_extra_trees = 1 - accuracy_score(y_test_all, y_pred_extra_trees_test)\n",
    "    test_mse_extra_trees = mean_squared_error(y_test_all, y_pred_extra_trees_test)\n",
    "\n",
    "\n",
    "    return {\n",
    "        'knn': {\n",
    "            'training_mse': np.mean(mse_list_knn),\n",
    "            'training_avg_error': 1 - np.mean(accuracy_list_knn),\n",
    "            'test_error': test_error_knn,\n",
    "            'test_mse': test_mse_knn\n",
    "        },\n",
    "        'neural_network': {\n",
    "            'training_mse': np.mean(mse_list_neural_network),\n",
    "            'training_avg_error': 1 - np.mean(accuracy_list_neural_network),\n",
    "            'test_error': test_error_neural_network,\n",
    "            'test_mse': test_mse_neural_network\n",
    "        },\n",
    "        'extra_trees': {\n",
    "            'training_mse': np.mean(mse_list_extra_trees),\n",
    "            'training_avg_error': 1 - np.mean(accuracy_list_extra_trees),\n",
    "            'test_error': test_error_extra_trees,\n",
    "            'test_mse': test_mse_extra_trees\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wine_quality():\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "    data = requests.get(url).text\n",
    "    df = pd.read_csv(StringIO(data), sep=';')\n",
    "    X = df.drop('quality', axis=1).values\n",
    "    y = df['quality'].values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def load_energy_efficiency():\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\"\n",
    "    df = pd.read_excel(url)\n",
    "    X = df.drop(['Y1', 'Y2'], axis=1).values\n",
    "    # Y1 Heating Load\n",
    "    y = df['Y1'].values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def load_spam():\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "    data = requests.get(url).text\n",
    "    df = pd.read_csv(StringIO(data), header=None)\n",
    "    X = df.drop(57, axis=1).values\n",
    "    y = df[57].values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def load_heart_disease():\n",
    "    # Cleveland Dataset\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "    df = pd.read_csv(url, header=None, na_values='?')\n",
    "    df = df.dropna()\n",
    "    X = df.drop(13, axis=1).values\n",
    "    y = df[13].values\n",
    "    return X, y\n",
    "\n",
    "def load_handwritten_digits():\n",
    "    digits = load_digits()\n",
    "    df = pd.DataFrame(digits.data, columns=digits.feature_names)\n",
    "    df['target'] = digits.target\n",
    "\n",
    "    X = df.drop('target', axis=1).values\n",
    "    y = df['target'].values\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vivek/opt/anaconda3/envs/jupyterenv/lib/python3.8/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 9 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "2023-04-23 16:50:04.160675: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "10/10 [==============================] - 0s 4ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "WARNING:tensorflow:5 out of the last 45 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fc0f99070d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "WARNING:tensorflow:6 out of the last 46 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fc0e17a1040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "X_wine, y_wine = load_wine_quality()\n",
    "X_energy, y_energy = load_energy_efficiency()\n",
    "X_spam, y_spam = load_spam()\n",
    "X_heart, y_heart = load_heart_disease()\n",
    "X_digits, y_digits = load_handwritten_digits()\n",
    "\n",
    "# Scale datasets\n",
    "scaler = MinMaxScaler()\n",
    "X_wine_scaled = scaler.fit_transform(X_wine)\n",
    "X_energy_scaled = scaler.fit_transform(X_energy)\n",
    "X_spam_scaled = scaler.fit_transform(X_spam)\n",
    "X_heart_scaled = scaler.fit_transform(X_heart)\n",
    "X_digits_scaled = scaler.fit_transform(X_digits)\n",
    "\n",
    "# Run classifiers for each dataset\n",
    "results_wine = run_classifiers(X_wine_scaled, y_wine, output = 9)\n",
    "#results_energy = run_classifiers(X_energy_scaled, y_energy)\n",
    "results_spam = run_classifiers(X_spam_scaled, y_spam)\n",
    "results_heart = run_classifiers(X_heart_scaled, y_heart)\n",
    "results_digits = run_classifiers(X_digits_scaled, y_digits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wine Quality\n",
      "Knn: Training MSE: 0.6904, Training Average Error: 0.4355, Test MSE: 0.6844, Test Error: 0.4719\n",
      "Neural_network: Training MSE: 0.4972, Training Average Error: 0.3855, Test MSE: 0.4750, Test Error: 0.4187\n",
      "Extra_trees: Training MSE: 0.4175, Training Average Error: 0.3112, Test MSE: 0.4031, Test Error: 0.3313\n",
      "\n",
      "Spam\n",
      "Knn: Training MSE: 0.1022, Training Average Error: 0.1022, Test MSE: 0.1216, Test Error: 0.1216\n",
      "Neural_network: Training MSE: 0.0584, Training Average Error: 0.0584, Test MSE: 0.0445, Test Error: 0.0445\n",
      "Extra_trees: Training MSE: 0.0467, Training Average Error: 0.0467, Test MSE: 0.0434, Test Error: 0.0434\n",
      "\n",
      "Heart Disease\n",
      "Knn: Training MSE: 1.3335, Training Average Error: 0.4507, Test MSE: 1.3000, Test Error: 0.3167\n",
      "Neural_network: Training MSE: 0.9583, Training Average Error: 0.4513, Test MSE: 1.0500, Test Error: 0.3333\n",
      "Extra_trees: Training MSE: 1.0942, Training Average Error: 0.4346, Test MSE: 1.2500, Test Error: 0.3500\n",
      "\n",
      "Handwritten Digits\n",
      "Knn: Training MSE: 0.3371, Training Average Error: 0.0118, Test MSE: 0.3667, Test Error: 0.0194\n",
      "Neural_network: Training MSE: 0.4799, Training Average Error: 0.0257, Test MSE: 0.6167, Test Error: 0.0333\n",
      "Extra_trees: Training MSE: 0.4232, Training Average Error: 0.0181, Test MSE: 0.3722, Test Error: 0.0222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    'Wine Quality': results_wine,\n",
    "    'Spam': results_spam,\n",
    "    'Heart Disease': results_heart,\n",
    "    'Handwritten Digits': results_digits\n",
    "}\n",
    "\n",
    "models = ['knn', 'neural_network', 'extra_trees']\n",
    "\n",
    "for dataset_name, dataset_results in datasets.items():\n",
    "    print(dataset_name)\n",
    "    for model_name in models:\n",
    "        model_results = dataset_results[model_name]\n",
    "        print(f\"{model_name.capitalize()}: Training MSE: {model_results['training_mse']:.4f}, Training Average Error: {model_results['training_avg_error']:.4f}, Test MSE: {model_results['test_mse']:.4f}, Test Error: {model_results['test_error']:.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
